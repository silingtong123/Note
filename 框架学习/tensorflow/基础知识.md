- TURE POSTIVE: 真阳性: 将正样本预测为正样本的数量（或概率）
- TURE NATION: 真阴性：负样本预测为负样本的数量（或概率）
- FALSE NATIVE: 假阳性： 正样本预测为负样本的数量（或概率）
- FALSE POSTIVE: 假阴性：负样本预测为正样本的数量（或概率）
- 常见求导公式：
  - (a)' = 0
  - ($x^a$)' = a($(x)^{a-1}$)
  - (sin$x$)' = cos$x$
  - (cos$x$)' = -sin$x$
  - ($a^x$)' = ($a^x$)'ln$a$
  - ($e^x$) = $e^x$
  - ($log_ax$) = 1/($xln_a$)
  - ($ln_x$) = 1/x
  - ($u+-v$)' = $u$' +-$v$'
  - ($uv$)' = $u'v$ + $uv'$
  - ($u/v$)' = ($(u'v - uv')/v^2$)
  - ($cu$)' = $cu'$
- 期望与方差,$E(X)$为期望，$D(X)$为方差
  - 期望是针对于随机变量而言的一个量,是针对于他的样本空间而言的,均值是一个统计量(对观察样本的统计)，期望是一种概率论概念，是一个数学特征。
  - 方差表示随机变量取值的分散性的一个数字特征，方差越大，说明随机变量的取值分布越不均匀，变化性越强
  - 离散型：$E(X)=\sum_{k=1}^{n}x_kp_k$, $D(X)=\sum_{i=1}^{n}(x_i-E(X))^2$
  - 连续型：设连续性随机变量X的概率密度函数为f(x)，若积分绝对收敛，则称积分的值为随机变量的数学期望，记为E(X)。$E(X)=\int^{+\infty}_{-\infty}xf(x)dx$,$D(X)=\int^{+\infty}_{-\infty}(x-E(X))^2f(x)dx$

- Word embeding(词嵌入): one-hot， 嵌入到低纬空间，高维词向量嵌入到一个低维空间，即降维。 
  - 2*6的one_hot变为了2*3的矩阵，其实可以变为2*2，即2*6 x 6*2 = 2*2 ，这个6*2矩阵即为embeding weight,又称为look up table。此时任何一个单词的onehot乘以这个矩阵都将得到自己的词向量，有了look up table就可以免去训练过程直接查表得到单词的词向量了
  - 当然也可以升维：2*6 x 6*200 = 2*200 升维时，可能把一些其他特征给放大了，或者把笼统的特征给分开。

- 深度学习中add, concatenate,elementwise
  - add的特征结合方式使得描述图像的特征下的信息量增多了，但是描述图像的维度本身并没有增加，只是每一维下的信息量在增加
  - 该方式是通道数的合并，也就是说描述图像本身的特征数（通道数）增加了，而每一特征下的信息是没有增加的
  - elementwise multiplication: 每一个输入向量"v"乘以一个给定的权重"w"向量

- product 乘积
- MLP和DNN区别
  - FC Layer（全连接，FullyConnected）：其每个节点都与上一层的所有节点相连，他不一定要求有隐藏层
  - MLP(多层感知机): 是一种特殊的全连接神经网络，它由输入层、若干个隐藏层和输出层组成。每个神经元都与上一层的所有神经元相连 
  - DNN（）：特指网络层数很深的网络。通常包括多个卷积layer和FC layer等其他layer
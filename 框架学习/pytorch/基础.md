### 编译安装
- 查看torch-torchvision-torchaudio匹配版本：https://pytorch.org/get-started/locally/
- 查看torch-torchvision-torchaudio历史匹配版本https://pytorch.org/get-started/previous-versions/
- threestudio环境安装py3.10：
  - ImportError: libGL.so.1: cannot open shared object file: No such file or directory
  - apt-get update && apt-get install ffmpeg libsm6 libxext6  -y
- MMVC pytorch版本对应: https://mmcv.readthedocs.io/zh_CN/latest/get_started/installation.html#id1
### 常用命名空间
- at 代表 ATen (A Tensor Library)，负责声明和定义Tensor运算相关的逻辑，是pytorch扩展c++接口中最常用到的命名空间
- c10 (Caffe Tensor Library)其实是 ATen 的基础，包含了PyTorch的核心抽象、Tensor和Storage数据结构的实际实现
- torch 命名空间下定义的 Tensor 相比于ATen 增加自动求导功能，但 c++ 扩展中一般不常见

### pybind11
- python -> C++转换： 从Python到C，要用到PyArg_ParseTuple()函数，从C到Python要用到Py_BuildValue()函数
  - 包含python头文件。#include "Python.h" 
  - 为每一个模块函数添加形如PyObject* Module_func()的封装函数
  - 为每一个模块函数添加一个PyMethodDef ModuleMethods[]数组/表
  - 添加模块初始化函数void initModule()
- PyArg_ParseTuple(args,format,...)
  - "b" (integer) [char] python整数->char中的tiny int
  - "h" (integer) [short int]  python整数->C语言short int。
  - "i" (integer) [int] python整数->C语言int
  - "l" (integer) [long int] python整数->long int
  - "f" (float) [float] python float->float
  - "d" (float) [double] python float->double
  - "D" (complex) [Py_complex] python 复数转换为C语言Py_complex结构
  - "O" (object) [PyObject *] 在C对象指针中存储Python对象(不进行任何转换)
  - "O!" (object)[typeobject, PyObject *] 这类似于“O”，但是接受两个C参数:第一个是Python类型对象的地址，第二个是对象指针存储在其中的C变量(类型为PyObject *)的地址
  - "O&" (object)[converter,anything] 通过转换器函数将Python对象转换为C变量。这需要两个参数:第一个是函数，第二个是C变量(任意类型)的地址
  - "S" (string) [PyStringObject *]：在C对象指针中存储Python string
  - "U" (Unicode string) [PyUnicodeObject *]：在C对象指针中存储Python Unicode对象
  - "s" (string or Unicode object) [char *]：将 python string or Unicode object -》 c char *
  - "s#" (string,Unicode or any read buffer compatible object) [char *, int]：“s”上的这个变体存储在两个C变量中，第一个变量是指向字符串的指针，第二个变量是字符串的长度
  - "z" (string or None) [char *]：与“s”类似，但是Python对象也可能是None，在这种情况下，C指针被设置为NULL
  - "z#" (string or None or any read buffer compatible object) [char *, int]：类似s和s#的关系
  - "u" (Unicode object) [Py_UNICODE *]将Python Unicode对象转换为指向16位Unicode (UTF-16)数据的空端缓冲区的C指针
  - "t#" (read-only character buffer) [char *, int]：与“s#”类似，但接受任何实现只读缓冲区接口的对象。char *变量设置为指向缓冲区的第一个字节，int设置为缓冲区的长度
  - "w" (read-write character buffer) [char *]：类似于“s”，但接受任何实现读写缓冲区接口的对象。调用者必须通过其他方法确定缓冲区的长度，或者使用“w#”
  - "w#" (read-write character buffer) [char *, int]：与“s#”类似，但接受任何实现读写缓冲区接口的对象。char *变量设置为指向缓冲区的第一个字节，int设置为缓冲区的长度
  - "|" 指示Python参数列表中的其余参数是可选的
  - ":"格式单元列表在此结束;冒号后面的字符串用作错误消息中的函数名
  - ";" 格式单元列表在此结束;冒号后面的字符串用作错误消息，而不是默认错误消息
-  pybind.cpp
-  ```C++
   class CppFunction{
     c10::optional<c10::DispatchKey> dispatch_key_;
     c10::KernelFunction func_; // 类似std::function
     c10::optional<c10::impl::CppSignature> cpp_signature_;
     std::unique_ptr<c10::FunctionSchema> schema_;
     std::string debug_;   
   }

   class BoxedKernel { //类似std::function
      c10::intrusive_ptr<OperatorKernel> functor_;
      InternalBoxedKernelFunction* boxed_kernel_func_;   
   } 

   TORCH_LIBRARY_IMPL(myops, CPU, m) // m is a torch::Library, myops is namespace
   ```
- PYBIND11_MODULE(module_name, m):第一个参数为module_name,第二个为是py::module类型

### TorchScript
- intrusive_ptr视为类似于std::shared_ptr的智能指针,使用此智能指针的原因是为了确保在语言(C ++，Python 和 TorchScript）之间对对象实例进行一致的生命周期管理

### FLOPs和FLOPS区别
- FLOPs: 指浮点运算次数,通常用来衡量神经网络的计算复杂度
- FLOPS: 指每秒钟可以执行的浮点运算次数, FLOPS也可以用来衡量计算机系统的性能
### C++ inference
- CMakeLists.txt:
```
cmake_minimum_required(VERSION 3.0 FATAL_ERROR)
project(custom_ops)

find_package(Torch REQUIRED)

add_executable(example-app example-app.cpp)
target_link_libraries(example-app "${TORCH_LIBRARIES}")
set_property(TARGET example-app PROPERTY CXX_STANDARD 14)
```
- 编译命令:使用libtorch或者libtorch_cuda
```
mkdir build
cd build
cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch .. / 
cmake --build . --config Release
```

### 加载保存模型
- 第一种（推荐）：只保存了模型参数，没有保存模型结构，加载模型时需要初始化模型结构，但比较灵活可以只加载部分参数
  - torch.save(model.state_dict(), 'model_path.pth')
  - model.load_state_dict(torch.load('model_path.pth'))
- 第二种：模型结构和参数全部都保存了，而且加载模型时不需要初始化模型结构
  - torch.save(model, 'model.pth')
  - model = torch.load('model.pth')

### 模型结构打印
- print(model): 打印模型结构
- model.named_parameters()：layer_name : layer_param存为元组放入list,只保存可学习、可被更新的参数,该函数返回迭代器，必须使用 for k,v遍历，获取layer-name和layer-param
- parameters()：只有后者layer-param
- state_dict()：layer_name : layer_param存为dict,且包含的所有layer中的所有参数，

### RNN
用于处理序列数据的经典模型, 有以下缺点：
- Sequential operations的复杂度随着序列长度的增加而增加，下一步必须等待上一步完成，等待时间为$O(N)$
- Maximum Path length的复杂度随着序列长度的增加而增加, 前后token距离越来越大，距离为$O(N)$, 越远的token信息越难被后面的token看见

RNN结构：
- rnn: x -> H1 -> O     x为输入，H1 = x * U, H1为隐藏层输出，U为输入到隐藏层的权重，O = H1 * V, O为输出层输出，V为隐藏层到输出层的权重 ，到此时还是普通模型；但rnn中 H1 = x* U + W * H1'  W就是上一次隐藏层的输出H1'到这次输出的权重（Note: 为了简单说明问题，偏置都没有包含在公式里面）
- Encoder-Decoder结构的rnn: 它先通过一个Encoder rnn网络读入所有的待翻译句子中的单词，得到一个包含原文所有信息的中间隐藏层，接着把中间隐藏层状态输入Decoder rnn网络，一个词一个词的输出翻译句子;缺陷是中间状态由于来自于输入网络最后的隐藏层，一般来说它是一个大小固定的向量，所以句子越长，需要表达的信息越多，固定向量会越丢失信息
  
优化方向attention，主要解决以下问题：
- 提高并行计算能力
- 每个token不损失信息地看见序列里的其他tokens

### Attention
根据Encoder-Decoder结构的rnn做改进，我们把Encoder隐藏层记为$h^{t}$, Decoder的隐藏层记为$H^t$,第t个输出词记为$y^t$
- 原来Deconder公式为：$H^t = f(H^{t-1},y^{t-1})$，$y^0=0$,$H^0=h^t$.
- 但我们要使得网络在翻译不同的句子时可以注意到并利用原文中不同的词语和语句段，新公式为：$H^t = f(H^{t-1},y^{t-1}，C^t)$, $C^t$为t时刻的上下文向量，一般是隐藏层值$h^t$的加权平均：$C^t = \sum_{T_x}^{i=1}a_{ti}h^t$,而如何分配权重就体现了输出这个$H^t$的时候应该给哪些原文词语更高的注意力.
- $a_k$很好的体现了在计算第$k$个输出词语时，应该给所有的怎样的权重$h^t$
- $a_k$如何确定:使用一个小神经网络计算出来, $a_{kt}$表示输出第k个词时，应该给$h^t$多少权重：     
  - $e_{ij} =score(H^{i-1}, h^j)$ 
  - $a_{ij}=\frac{exp(e_{ij})}{\sum_{k=1}^{T_x}exp(e_{ik})}$ 等价于$a_i=SoftMax(e_i)$
  - 一般$score(H^{i-1}, h^j)$可以为加法模型，乘法模型，点积模型，缩放点积模型
- attention分类：
  - soft attention就是前面写的
  - Hard attention不使用加权平均的方法，对于每个待选择的输入隐藏层，它要么完全采纳进入上下文向量，要么彻底抛弃，选择隐藏层的两种方式
    - 直接选择最高权重（$a_{ki}$）的隐藏层
    - 按照 $a_{ki}$的分布来对隐藏层进行随机采样获得一个隐藏层
  - 前面两者都属于GLobal attention，他存在一个问题：文章太长时太多的输入候选隐藏层可能会降低算法的运行速度，所以我们可以挑选出可能比较有用的一批隐藏层，只对它们进行Attention计算
  - Self Attention：自监督，多用于非seq2seq任务，通常Encoder-Decoder网络只能构建在seq2seq任务中，从Hhc到QKV
    - Decoder的隐藏层记为$H^t$命名为$Q$,代表查询（Query）,Encoder隐藏层记为$h^{t}$命名为$K$,代表查询的目标键（Key); 如果把这个过程比喻成使用Query在数据库中按照Key进行筛选，那应该还差一个Key对应的值Value，这里我们还没有定义Value是什么: NLP的一般应用中，查询的Key和Value是同一个东西,但Self-Attention我们的Key和Value不是同一个值，记Encoder网络中第t步得到的键为$k_t$，值为 $v_t$，我们的权重计算函数就可以写作:
      - $Attention((K,V),q)=\sum_{i=1}^{N}a_iv_i$
      - Note: 我们按照Key分配权重但是最终把Value进行加权平均

### transformer
- 基础是attention机制, attention是为了解决rnn无法解决的的问题,即第k个token必须依赖前k-1个token结果，但attention中的token是同时生成的；attention是为了解决普通模型只能单独处理一个个输入的问题,增大并行度
- transformer：
  - $a_{ij}=SoftMax(\frac{Q_iK_i^T}{\sqrt{d_k}})$: 矩阵相乘表示一个矩阵在另一个矩阵上的投影，表示相关性，而${Q_iK_i^T}$，就表示了${Q_i}$和${K_i^T}$的相关性，使用softmax是归一化，实验${\sqrt{d_k}})$的目的将方差和将${d_k}$解耦
  - $A_i=Attention(Q_i,(K,V))=\sum_{j_1}^{d_k}a_{ij}V_j$
  - $a_{ij}$ 代表了在第i个单词的眼中，应该给第$j$个单词的$K_j$多少权重,分母为缩放防止softmax梯度过小，$d_k$为key的长度；$Q_i$,$K_i$,$V_i$都是把$X_i$进行矩阵乘得到的向量，其对应的学习向量为$W^Q$,$W^K$,$W^V$
  - Multi-Head Attention 简单来说多头Attention就是把Self-Attention使用不一样的参数算了很多次,最后把它们加起来就是多头Attention,经过Muilt-Head Attention层，我们的attention输出A 和输入X有着一样的大小，后面加上残差连接和Layer Norm就是完整的Encoder
  - 加上layer norm: $LayerNorm(A)=\frac{W^g}{a}\cdot(A-u)+bias$, 
    - $a=\frac{1}{T_x}\sum_{i=1}^{T_x}A_i$,即为平均数，
    - $u=\sqrt{\frac{1}{T_x}\sum_{i=1}^{T_x}(A_i-a)^2}$为方差，A为attention
  -  

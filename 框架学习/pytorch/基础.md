### 编译安装
- 查看torch-torchvision-torchaudio匹配版本：https://pytorch.org/get-started/locally/
- 查看torch-torchvision-torchaudio历史匹配版本https://pytorch.org/get-started/previous-versions/
- threestudio环境安装py3.10：
  - ImportError: libGL.so.1: cannot open shared object file: No such file or directory
  - apt-get update && apt-get install ffmpeg libsm6 libxext6  -y
- MMVC pytorch版本对应: https://mmcv.readthedocs.io/zh_CN/latest/get_started/installation.html#id1
### 常用命名空间
- at 代表 ATen (A Tensor Library)，负责声明和定义Tensor运算相关的逻辑，是pytorch扩展c++接口中最常用到的命名空间
- c10 (Caffe Tensor Library)其实是 ATen 的基础，包含了PyTorch的核心抽象、Tensor和Storage数据结构的实际实现
- torch 命名空间下定义的 Tensor 相比于ATen 增加自动求导功能，但 c++ 扩展中一般不常见

### 转换onnx的自定义算子
[文档参考](https://zhuanlan.zhihu.com/p/477743341)
- PyTorch 模型转一个这样的 TorchScript 模型，有跟踪（trace）和记录（script）两种导出计算图的方法
  - 跟踪法只能通过实际运行一遍模型的方法导出模型的静态图，即无法识别出模型中的控制流（如循环）
  - 记录法则能通过解析模型来正确记录所有的控制流
- 使模型在 ONNX 转换时有不同的行为
  - torch.onnx.is_in_onnx_export() 可以实现这一任务
- [onnx 算子文档](https://github.com/onnx/onnx/blob/main/docs/Operators.md)
- torch中ONNX有关的定义全部放在[torch.onnx](https://github.com/pytorch/pytorch/tree/master/torch/onnx)
- 符号函数：
  - 符号函数，可以看成是 PyTorch 算子类的一个静态方法。在把 PyTorch 模型转换成 ONNX 模型时，各个 PyTorch 算子的符号函数会被依次调用，以完成 PyTorch 算子到 ONNX 算子的转换
  - 函数定义如下：
  ```py
  def symbolic(g: torch._C.Graph, input_0: torch._C.Value, input_1: torch._C.Value, ...): 
    # 第一个参数就固定叫 g，它表示和计算图相关的内容；后面的每个参数都表示算子的输入，需要和算子的前向推理接口的输入相同
  ```
- pytorch算子到onnx算子转换的三个保证：
  - 算子在 PyTorch 中有实现
  - 有把该 PyTorch 算子映射成一个或多个 ONNX 算子的方法
  - ONNX 有相应的算子
- 针对三个保证的三个解决问题思路：
  - 没有PyTorch 的算子实现，有以下几种方法实现torch算子：
    - 组合现有torch 算子
    - 添加TorchScript 算子
    - 添加普通C++ 拓展算子
  - 没有映射关系，有以下几种方式添加映射：
    - 为 ATen 算子添加符号函数，此时算子在ATen中已实现，且onnx算子存在，只缺少映射
    - 为 TorchScript 算子添加符号函数，此时算子在ATen中无实现，且onnx算子存在，还缺少映射
    - 实现torch的cpp算子并封装成 torch.autograd.Function 并添加符号函数，此时算子在ATen中无实现，且onnx算子存在，还缺少映射
  - 没有onnx算子：
    - 使用现有的onnx算子组合
    - 定义新的算子
- 针对映射关系解决
  - 为ATen 算子添加符号函数：此时算子在ATen中已实现，且onnx算子存在，只缺少映射
    - torch/_C/_VariableFunctions.pyi 和 torch/nn/functional.pyi 找算子名，asinh为例，可在torch/_C/_VariableFunctions.pyi找到asinh的接口定义
    - 获取目标 ONNX 算子的定义，参考[onnx 算子文档](https://github.com/onnx/onnx/blob/main/docs/Operators.md)
    - PyTorch 算子转换成 ONNX 算子时，需要在符号函数中调用g.op方法来为最终的计算图添加一个 ONNX 算子
    ```py
    def op(name: str, input_0: torch._C.Value, input_1: torch._C.Value, ...) 
    # 第一个参数是算子名称。如果该算子是普通的 ONNX 算子，只需要把它在 ONNX 官方文档里的名称填进去即可

    from torch.onnx.symbolic_registry import register_op 
 
    # op_symbolic 表示改op的静态方法
    def asinh_symbolic(g, input, *, out=None): 
       return g.op("Asinh", input) 
 
    # 将定义的onnx op Asinh 和torch中的asinh op，通过asinh_symbolic函数绑定起来
    register_op('asinh', asinh_symbolic, '', 9) 

    # register_op第一个参数是目标 ATen 算子名，第二个是要注册的符号函数，这两个参数很好理解。第三个参数是算子的“域”，对于普通 ONNX 算子，直接填空字符串即可。第四个参数表示向哪个算子集版本注册
    ```
  - 自定义torch算子：对于一些比较复杂的运算，仅使用 PyTorch 原生算子是无法实现的，官方推荐使用[添加 TorchScript 算子](https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html)
    - 文档中跳过新增 TorchScript 算子，因为较繁琐？？直接使用torchvision中定义好的torchscript算子deform_conv2d，来观察整个流程
    - g.op自定义 ONNX 算子，名称前必须加命名空间，否则算子会被默认成 ONNX 的官方算子：
    ```py
    @parse_args("v", "v", "v", "v", "v", "i", "i", "i", "i", "i", "i", "i", "i", "none") 
    def symbolic(g,  
            input, 
            weight, 
            offset, 
            mask, 
            bias, 
            stride_h, stride_w, 
            pad_h, pad_w, 
            dil_h, dil_w, 
            n_weight_grps, 
            n_offset_grps, 
            use_mask): 
        return g.op("custom::deform_conv2d", input, offset) 

    register_custom_op_symbolic("torchvision::deform_conv2d", symbolic, 9)  
    #  torchvision::deform_conv2d 表示是torchvision中的算子，和onnx中custom::deform_conv2d算子绑定

    ```
  - torch.autograd.Function 作者推荐，以my_add为例，该算子实现2*a + b：
    - cpp 源文件实现
      - ```cpp
        // my_add.cpp 
     
        #include <torch/torch.h> 
         
        torch::Tensor my_add(torch::Tensor a, torch::Tensor b) 
        { 
            return 2 * a + b; 
        } 
         
        PYBIND11_MODULE(my_lib, m) 
        { 
            m.def("my_add", my_add); 
        } 
        ```
    - 添加steup.py 编译源文件
      - ```py
        from setuptools import setup 
        from torch.utils import cpp_extension 
         
        setup(name='my_add', 
              ext_modules=[cpp_extension.CppExtension('my_lib', ['my_add.cpp'])], 
              cmdclass={'build_ext': cpp_extension.BuildExtension}) 
        ```
    - 使用命令自动编译：python setup.py develop
    - 用 torch.autograd.Function 封装
      - ```py
        import torch 
        import my_lib 
        class MyAddFunction(torch.autograd.Function): 
         
            @staticmethod 
            def forward(ctx, a, b): 
                return my_lib.my_add(a, b) 
         
            @staticmethod 
            def symbolic(g, a, b): 
                two = g.op("Constant", value_t=torch.tensor([2])) 
                a = g.op('Mul', a, two) 
                return g.op('Add', a, b) 
        
        # 使用举例， my_add为apply函数别名
        my_add = MyAddFunction.apply 
 
        class MyAdd(torch.nn.Module): 
            def __init__(self): 
                super().__init__() 
         
            def forward(self, a, b): 
                return my_add(a, b)         
        ```
### torch C++
- python端
```py
from torch.nn import functional as F

def net(params, x):
    x = F.linear(x, params[0], params[1])
    x = F.relu(x)

    x = F.linear(x, params[2], params[3])
    x = F.relu(x)

    x = F.linear(x, params[4], params[5])
    return x

    # F.linear为cpp算子
```
- cpp端： torch\csrc\api\src\nn\modules\linear.cpp （python端标准算子的入口torch\csrc\api\src）
```cpp
//调用Aten空间里的逻辑
Tensor LinearImpl::forward(const Tensor& input) {
  return F::linear(input, weight, bias);
}

//声明算子: TORCH_MODULE(Linear);
```
### pybind11
- python -> C++转换： 从Python到C，要用到PyArg_ParseTuple()函数，从C到Python要用到Py_BuildValue()函数
  - 包含python头文件。#include "Python.h" 
  - 为每一个模块函数添加形如PyObject* Module_func()的封装函数
  - 为每一个模块函数添加一个PyMethodDef ModuleMethods[]数组/表
  - 添加模块初始化函数void initModule()
- PyArg_ParseTuple(args,format,...)
  - "b" (integer) [char] python整数->char中的tiny int
  - "h" (integer) [short int]  python整数->C语言short int。
  - "i" (integer) [int] python整数->C语言int
  - "l" (integer) [long int] python整数->long int
  - "f" (float) [float] python float->float
  - "d" (float) [double] python float->double
  - "D" (complex) [Py_complex] python 复数转换为C语言Py_complex结构
  - "O" (object) [PyObject *] 在C对象指针中存储Python对象(不进行任何转换)
  - "O!" (object)[typeobject, PyObject *] 这类似于“O”，但是接受两个C参数:第一个是Python类型对象的地址，第二个是对象指针存储在其中的C变量(类型为PyObject *)的地址
  - "O&" (object)[converter,anything] 通过转换器函数将Python对象转换为C变量。这需要两个参数:第一个是函数，第二个是C变量(任意类型)的地址
  - "S" (string) [PyStringObject *]：在C对象指针中存储Python string
  - "U" (Unicode string) [PyUnicodeObject *]：在C对象指针中存储Python Unicode对象
  - "s" (string or Unicode object) [char *]：将 python string or Unicode object -》 c char *
  - "s#" (string,Unicode or any read buffer compatible object) [char *, int]：“s”上的这个变体存储在两个C变量中，第一个变量是指向字符串的指针，第二个变量是字符串的长度
  - "z" (string or None) [char *]：与“s”类似，但是Python对象也可能是None，在这种情况下，C指针被设置为NULL
  - "z#" (string or None or any read buffer compatible object) [char *, int]：类似s和s#的关系
  - "u" (Unicode object) [Py_UNICODE *]将Python Unicode对象转换为指向16位Unicode (UTF-16)数据的空端缓冲区的C指针
  - "t#" (read-only character buffer) [char *, int]：与“s#”类似，但接受任何实现只读缓冲区接口的对象。char *变量设置为指向缓冲区的第一个字节，int设置为缓冲区的长度
  - "w" (read-write character buffer) [char *]：类似于“s”，但接受任何实现读写缓冲区接口的对象。调用者必须通过其他方法确定缓冲区的长度，或者使用“w#”
  - "w#" (read-write character buffer) [char *, int]：与“s#”类似，但接受任何实现读写缓冲区接口的对象。char *变量设置为指向缓冲区的第一个字节，int设置为缓冲区的长度
  - "|" 指示Python参数列表中的其余参数是可选的
  - ":"格式单元列表在此结束;冒号后面的字符串用作错误消息中的函数名
  - ";" 格式单元列表在此结束;冒号后面的字符串用作错误消息，而不是默认错误消息
-  pybind.cpp
-  ```C++
   class CppFunction{
     c10::optional<c10::DispatchKey> dispatch_key_;
     c10::KernelFunction func_; // 类似std::function
     c10::optional<c10::impl::CppSignature> cpp_signature_;
     std::unique_ptr<c10::FunctionSchema> schema_;
     std::string debug_;   
   }

   class BoxedKernel { //类似std::function
      c10::intrusive_ptr<OperatorKernel> functor_;
      InternalBoxedKernelFunction* boxed_kernel_func_;   
   } 

   TORCH_LIBRARY_IMPL(myops, CPU, m) // m is a torch::Library, myops is namespace
   ```
- PYBIND11_MODULE(module_name, m):第一个参数为module_name,第二个为是py::module类型

### TorchScript
- intrusive_ptr视为类似于std::shared_ptr的智能指针,使用此智能指针的原因是为了确保在语言(C ++，Python 和 TorchScript）之间对对象实例进行一致的生命周期管理

### FLOPs和FLOPS区别
- FLOPs: 指浮点运算次数,通常用来衡量神经网络的计算复杂度
- FLOPS: 指每秒钟可以执行的浮点运算次数, FLOPS也可以用来衡量计算机系统的性能
- MACs: 乘法累加运算的次数, 一般来说FLOPs是MACs的两倍，前者将乘加都算了
- $Y=XW$: 假设X(B,I),W(I,O),生成Y的一个元素需要计算需要X的某一行和W的某一列相乘和累加，理论需要I次乘法和I-1次加法，Y中一共 $B*O$个元素，所以FLOPs为$B * O *(I+(I-1)) = B*O(2I-1)$,如果有bias ,那么生成Y时需要一个额外的加法，所以$FLOPs = B*2IO$

```
params per gpu:                                               4884.09 M
params of model = params per GPU * mp_size:                   0
fwd MACs per GPU:                                             118.38 GMACs
fwd flops per GPU:                                            236.77 G
fwd flops of model = fwd flops per GPU * mp_size:             236.77 G
fwd latency:                                                  2.47 s
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:          95.89 GFLOPS
```
- ds_config如下，[deepspeed profile文档](https://www.deepspeed.ai/tutorials/flops-profiler/)
```json
{
  "flops_profiler": {
    "enabled": false,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null,
    }
}
```

### C++ inference
- CMakeLists.txt:
```
cmake_minimum_required(VERSION 3.0 FATAL_ERROR)
project(custom_ops)

find_package(Torch REQUIRED)

add_executable(example-app example-app.cpp)
target_link_libraries(example-app "${TORCH_LIBRARIES}")
set_property(TARGET example-app PROPERTY CXX_STANDARD 14)
```
- 编译命令:使用libtorch或者libtorch_cuda
```
mkdir build
cd build
cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch .. / 
cmake --build . --config Release
```

### 加载保存模型
- 第一种（推荐）：只保存了模型参数，没有保存模型结构，加载模型时需要初始化模型结构，但比较灵活可以只加载部分参数
  - torch.save(model.state_dict(), 'model_path.pth')
  - model.load_state_dict(torch.load('model_path.pth')) model必须提前定义
  - 也可使用model = torch.load('model.pth')， model类型为dict，model.state_dict()会报错
  - 对于分片的权重，可以对不同的分片调用load函数，得到不同的dict, torch.load("pytorch_model-00001-of-00002.bin")
- 第二种：模型结构和参数全部都保存了，而且加载模型时不需要初始化模型结构
  - torch.save(model, 'model.pth')
  - model = torch.load('model.pth') model类型为Module，可model.state_dict()

### 模型结构打印
- print(model): 打印模型结构
- model.named_parameters()：layer_name : layer_param存为元组放入list,只保存可学习、可被更新的参数,该函数返回迭代器，必须使用 for k,v遍历，获取layer-name和layer-param
- parameters()：只有后者layer-param
- state_dict()：layer_name : layer_param存为dict,且包含的所有layer中的所有参数，

### RNN
用于处理序列数据的经典模型, 有以下缺点：
- Sequential operations的复杂度随着序列长度的增加而增加，下一步必须等待上一步完成，等待时间为$O(N)$
- Maximum Path length的复杂度随着序列长度的增加而增加, 前后token距离越来越大，距离为$O(N)$, 越远的token信息越难被后面的token看见

RNN结构：
- rnn: x -> H1 -> O     x为输入，H1 = x * U, H1为隐藏层输出，U为输入到隐藏层的权重，O = H1 * V, O为输出层输出，V为隐藏层到输出层的权重 ，到此时还是普通模型；但rnn中 H1 = x* U + W * H1'  W就是上一次隐藏层的输出H1'到这次输出的权重（Note: 为了简单说明问题，偏置都没有包含在公式里面）
- Encoder-Decoder结构的rnn: 它先通过一个Encoder rnn网络读入所有的待翻译句子中的单词，得到一个包含原文所有信息的中间隐藏层，接着把中间隐藏层状态输入Decoder rnn网络，一个词一个词的输出翻译句子;缺陷是中间状态由于来自于输入网络最后的隐藏层，一般来说它是一个大小固定的向量，所以句子越长，需要表达的信息越多，固定向量会越丢失信息
  
优化方向attention，主要解决以下问题：
- 提高并行计算能力
- 每个token不损失信息地看见序列里的其他tokens

### Attention
根据Encoder-Decoder结构的rnn做改进，我们把Encoder隐藏层记为$h^{t}$, Decoder的隐藏层记为$H^t$,第t个输出词记为$y^t$
- 原来Deconder公式为：$H^t = f(H^{t-1},y^{t-1})$，$y^0=0$,$H^0=h^t$.
- 但我们要使得网络在翻译不同的句子时可以注意到并利用原文中不同的词语和语句段，新公式为：$H^t = f(H^{t-1},y^{t-1}，C^t)$, $C^t$为t时刻的上下文向量，一般是隐藏层值$h^t$的加权平均：$C^t = \sum_{T_x}^{i=1}a_{ti}h^t$,而如何分配权重就体现了输出这个$H^t$的时候应该给哪些原文词语更高的注意力.
- $a_k$很好的体现了在计算第$k$个输出词语时，应该给所有的怎样的权重$h^t$
- $a_k$如何确定:使用一个小神经网络计算出来, $a_{kt}$表示输出第k个词时，应该给$h^t$多少权重：     
  - $e_{ij} =score(H^{i-1}, h^j)$ 
  - $a_{ij}=\frac{exp(e_{ij})}{\sum_{k=1}^{T_x}exp(e_{ik})}$ 等价于$a_i=SoftMax(e_i)$
  - 一般$score(H^{i-1}, h^j)$可以为加法模型，乘法模型，点积模型，缩放点积模型
- attention分类：
  - soft attention就是前面写的
  - Hard attention不使用加权平均的方法，对于每个待选择的输入隐藏层，它要么完全采纳进入上下文向量，要么彻底抛弃，选择隐藏层的两种方式
    - 直接选择最高权重（$a_{ki}$）的隐藏层
    - 按照 $a_{ki}$的分布来对隐藏层进行随机采样获得一个隐藏层
  - 前面两者都属于GLobal attention，他存在一个问题：文章太长时太多的输入候选隐藏层可能会降低算法的运行速度，所以我们可以挑选出可能比较有用的一批隐藏层，只对它们进行Attention计算
  - Self Attention：自监督，多用于非seq2seq任务，通常Encoder-Decoder网络只能构建在seq2seq任务中，从Hhc到QKV
    - Decoder的隐藏层记为$H^t$命名为$Q$,代表查询（Query）,Encoder隐藏层记为$h^{t}$命名为$K$,代表查询的目标键（Key); 如果把这个过程比喻成使用Query在数据库中按照Key进行筛选，那应该还差一个Key对应的值Value，这里我们还没有定义Value是什么: NLP的一般应用中，查询的Key和Value是同一个东西,但Self-Attention我们的Key和Value不是同一个值，记Encoder网络中第t步得到的键为$k_t$，值为 $v_t$，我们的权重计算函数就可以写作:
      - $Attention((K,V),q)=\sum_{i=1}^{N}a_iv_i$
      - Note: 我们按照Key分配权重但是最终把Value进行加权平均

### transformer
- 基础是attention机制, attention是为了解决rnn无法解决的的问题,即第k个token必须依赖前k-1个token结果，但attention中的token是同时生成的；attention是为了解决普通模型只能单独处理一个个输入的问题,增大并行度
- transformer：
  - $a_{ij}=SoftMax(\frac{Q_iK_i^T}{\sqrt{d_k}})$: 矩阵相乘表示一个矩阵在另一个矩阵上的投影，表示相关性，而${Q_iK_i^T}$，就表示了${Q_i}$和${K_i^T}$的相关性，使用softmax是归一化，实验${\sqrt{d_k}})$的目的将方差和将${d_k}$解耦
  - $A_i=Attention(Q_i,(K,V))=\sum_{j_1}^{d_k}a_{ij}V_j$
  - $a_{ij}$ 代表了在第i个单词的眼中，应该给第$j$个单词的$K_j$多少权重,分母为缩放防止softmax梯度过小，$d_k$为key的长度；$Q_i$,$K_i$,$V_i$都是把$X_i$进行矩阵乘得到的向量，其对应的学习向量为$W^Q$,$W^K$,$W^V$
  - Multi-Head Attention 简单来说多头Attention就是把Self-Attention使用不一样的参数算了很多次,最后把它们加起来就是多头Attention,经过Muilt-Head Attention层，我们的attention输出A 和输入X有着一样的大小，后面加上残差连接和Layer Norm就是完整的Encoder
  - 加上layer norm: $LayerNorm(A)=\frac{W^g}{a}\cdot(A-u)+bias$, 
    - $a=\frac{1}{T_x}\sum_{i=1}^{T_x}A_i$,即为平均数，
    - $u=\sqrt{\frac{1}{T_x}\sum_{i=1}^{T_x}(A_i-a)^2}$为方差，A为attention
    - layernorm是对单个特征对应的所有样本做归一化，batchnorm是对一个样本的所有特征最归一化
  -  
